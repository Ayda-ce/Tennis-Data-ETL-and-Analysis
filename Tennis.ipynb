{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6f841d7-6d38-4a1d-887d-6192f2544913",
   "metadata": {},
   "source": [
    "# Merging files and creating dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a02303d-27d4-4a09-b8ff-5970feea65c9",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaf19e6a-8103-41e0-b4eb-86df41c1f66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Built-in ---\n",
    "import calendar\n",
    "import gc\n",
    "import re\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Third-party ---\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML, display\n",
    "from scipy import stats\n",
    "from scipy.stats import mannwhitneyu, pearsonr, spearmanr, ttest_ind, f_oneway\n",
    "from tabulate import tabulate\n",
    "import warnings\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "# --- Settings ---\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32fd529-a0c6-483d-a380-ea81ef1517b8",
   "metadata": {},
   "source": [
    "### Step 1 : Define base class for processing parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4717e3d3-d180-47ab-bc36-769ca8144c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TennisDataProcessor:\n",
    "    def __init__(self, base_path):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.all_data = {}\n",
    "        self.failed_files = []\n",
    "        self.data_by_prefix = {}\n",
    "\n",
    "    def extract_file_components(self, filename):\n",
    "        \"\"\"Extract file components: prefix_number\"\"\"\n",
    "        match = re.match(r\"([a-zA-Z_]+)_(\\d+)\", filename)\n",
    "        if match:\n",
    "            prefix = match.group(1)\n",
    "            number = int(match.group(2))\n",
    "            return prefix, number\n",
    "        return filename, 0\n",
    "\n",
    "    def get_sorted_folders(self):\n",
    "        \"\"\"Get sorted list of folders\"\"\"\n",
    "        folders = [f for f in self.base_path.iterdir() if f.is_dir()]\n",
    "        folders_sorted = sorted(folders, key=lambda x: x.name)\n",
    "        return folders_sorted\n",
    "\n",
    "    def get_file_types_in_folder(self, folder_path):\n",
    "        \"\"\"Extract file types in a folder\"\"\"\n",
    "        parquet_files = list(folder_path.glob(\"*.parquet\"))\n",
    "\n",
    "        prefixes = set()\n",
    "        prefix_counts = {}\n",
    "\n",
    "        for file_path in parquet_files:\n",
    "            prefix, _ = self.extract_file_components(file_path.stem)\n",
    "            prefixes.add(prefix)\n",
    "\n",
    "            if prefix not in prefix_counts:\n",
    "                prefix_counts[prefix] = 0\n",
    "            prefix_counts[prefix] += 1\n",
    "\n",
    "        return prefixes, prefix_counts\n",
    "\n",
    "    def find_missing_files(self):\n",
    "        \"\"\"Find folders missing required files\"\"\"\n",
    "        folders = self.get_sorted_folders()\n",
    "\n",
    "        print(\"\\nüîç Checking missing files...\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # Collect all prefixes\n",
    "        all_prefixes = set()\n",
    "        folder_prefixes = {}\n",
    "\n",
    "        for folder in folders:\n",
    "            prefixes, _ = self.get_file_types_in_folder(folder)\n",
    "            folder_prefixes[folder.name] = prefixes\n",
    "            all_prefixes.update(prefixes)\n",
    "\n",
    "        # Find missing files\n",
    "        missing_report = {}\n",
    "\n",
    "        for prefix in sorted(all_prefixes):\n",
    "            folders_with_prefix = [f for f, p in folder_prefixes.items() if prefix in p]\n",
    "            folders_without_prefix = [\n",
    "                f for f, p in folder_prefixes.items() if prefix not in p\n",
    "            ]\n",
    "\n",
    "            if folders_without_prefix:\n",
    "                missing_report[prefix] = {\n",
    "                    \"found_in\": len(folders_with_prefix),\n",
    "                    \"missing_in\": folders_without_prefix,\n",
    "                }\n",
    "\n",
    "        # Show report\n",
    "        if missing_report:\n",
    "            print(\"\\n‚ö†Ô∏è Missing files:\")\n",
    "            for prefix, info in missing_report.items():\n",
    "                print(f\"\\nüìÅ '{prefix}':\")\n",
    "                print(f\"   ‚úì Present in {info['found_in']} folders\")\n",
    "                print(f\"   ‚úó Missing in folders:\")\n",
    "                for folder_name in info[\"missing_in\"][:5]:\n",
    "                    print(f\"      - {folder_name}\")\n",
    "                if len(info[\"missing_in\"]) > 5:\n",
    "                    print(f\"      ... and {len(info['missing_in']) - 5} more folders\")\n",
    "        else:\n",
    "            print(\"‚úÖ All files exist in all folders\")\n",
    "\n",
    "        # Check similar names\n",
    "        print(\"\\nüîç Checking similar names...\")\n",
    "        similar_names = self.find_similar_names(all_prefixes)\n",
    "        if similar_names:\n",
    "            print(\"\\n‚ö†Ô∏è Similar names found:\")\n",
    "            for group in similar_names:\n",
    "                print(f\"   ‚Ä¢ {', '.join(group)}\")\n",
    "\n",
    "        print(\"=\" * 80)\n",
    "        return missing_report\n",
    "\n",
    "    def find_similar_names(self, names):\n",
    "        \"\"\"Find similar names (possible typos)\"\"\"\n",
    "        from difflib import SequenceMatcher\n",
    "\n",
    "        similar_groups = []\n",
    "        processed = set()\n",
    "\n",
    "        for name1 in names:\n",
    "            if name1 in processed:\n",
    "                continue\n",
    "\n",
    "            group = [name1]\n",
    "            for name2 in names:\n",
    "                if name2 != name1 and name2 not in processed:\n",
    "                    similarity = SequenceMatcher(None, name1, name2).ratio()\n",
    "                    if similarity > 0.8:  # 80% similarity\n",
    "                        group.append(name2)\n",
    "                        processed.add(name2)\n",
    "\n",
    "            if len(group) > 1:\n",
    "                similar_groups.append(group)\n",
    "            processed.add(name1)\n",
    "\n",
    "        return similar_groups\n",
    "\n",
    "    def show_available_folders(self, show_count=10, show_file_types=True):\n",
    "        \"\"\"Display list of available folders with details\"\"\"\n",
    "        folders = self.get_sorted_folders()\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üìÅ Available folders:\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        for i, folder in enumerate(folders[:show_count], 1):\n",
    "            total_files = len(list(folder.glob(\"*.parquet\")))\n",
    "\n",
    "            print(f\"\\n{i:3}. üìÖ {folder.name}\")\n",
    "            print(f\"     üìä Total files: {total_files}\")\n",
    "\n",
    "            if show_file_types and total_files > 0:\n",
    "                prefixes, prefix_counts = self.get_file_types_in_folder(folder)\n",
    "\n",
    "                if prefixes:\n",
    "                    print(f\"     üìÇ File types ({len(prefixes)} types):\")\n",
    "\n",
    "                    sorted_prefixes = sorted(prefix_counts.items())\n",
    "                    max_items_per_line = 3\n",
    "\n",
    "                    for j in range(0, len(sorted_prefixes), max_items_per_line):\n",
    "                        line_items = sorted_prefixes[j : j + max_items_per_line]\n",
    "                        line_text = \"        \"\n",
    "                        for prefix, count in line_items:\n",
    "                            line_text += f\"‚Ä¢ {prefix}({count})  \"\n",
    "                        print(line_text)\n",
    "\n",
    "        if len(folders) > show_count:\n",
    "            print(f\"\\n     ... and {len(folders) - show_count} more folders\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"üìä Summary:\")\n",
    "        print(f\"   ‚Ä¢ Total folders: {len(folders)}\")\n",
    "        total_all_files = sum(len(list(f.glob(\"*.parquet\"))) for f in folders)\n",
    "        print(f\"   ‚Ä¢ Total files: {total_all_files:,}\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "        return folders\n",
    "\n",
    "    def inspect_parquet_file(\n",
    "        self, file_path=None, folder_idx=1, file_prefix=None, rows=5\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inspect content of a parquet file\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        file_path: direct file path\n",
    "        folder_idx: folder index\n",
    "        file_prefix: target file prefix\n",
    "        rows: number of rows to display\n",
    "        \"\"\"\n",
    "        if file_path:\n",
    "            # Using direct path\n",
    "            target_file = Path(file_path)\n",
    "        else:\n",
    "            # Finding file based on parameters\n",
    "            folders = self.get_sorted_folders()\n",
    "            if folder_idx > len(folders):\n",
    "                print(f\"‚ùå Folder number {folder_idx} does not exist\")\n",
    "                return None\n",
    "\n",
    "            folder = folders[folder_idx - 1]\n",
    "\n",
    "            if file_prefix:\n",
    "                # First file with this prefix\n",
    "                files = list(folder.glob(f\"{file_prefix}_*.parquet\"))\n",
    "                if not files:\n",
    "                    print(\n",
    "                        f\"‚ùå No file with prefix '{file_prefix}' found in folder {folder.name}\"\n",
    "                    )\n",
    "                    return None\n",
    "                target_file = files[0]\n",
    "            else:\n",
    "                # Select first file\n",
    "                files = list(folder.glob(\"*.parquet\"))\n",
    "                if not files:\n",
    "                    print(f\"‚ùå No parquet files found in folder {folder.name}\")\n",
    "                    return None\n",
    "                target_file = files[0]\n",
    "\n",
    "        print(f\"\\nüìÑ Inspecting file: {target_file.name}\")\n",
    "        print(f\"üìÅ From folder: {target_file.parent.name}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        try:\n",
    "            # Read file\n",
    "            df = pd.read_parquet(target_file)\n",
    "\n",
    "            # General info\n",
    "            print(f\"\\nüìä General Info:\")\n",
    "            print(f\"   ‚Ä¢ Rows: {len(df):,}\")\n",
    "            print(f\"   ‚Ä¢ Columns: {len(df.columns)}\")\n",
    "            print(\n",
    "                f\"   ‚Ä¢ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\"\n",
    "            )\n",
    "\n",
    "            # Column info\n",
    "            print(f\"\\nüìã Columns and Types:\")\n",
    "            for col in df.columns:\n",
    "                print(f\"   ‚Ä¢ {col}: {df[col].dtype}\")\n",
    "\n",
    "            # First rows\n",
    "            print(f\"\\nüîç Sample Data ({rows} rows):\")\n",
    "            print(df.head(rows).to_string())\n",
    "\n",
    "            # Descriptive stats\n",
    "            print(f\"\\nüìà Descriptive Statistics:\")\n",
    "            print(df.describe().to_string())\n",
    "\n",
    "            # Null values\n",
    "            null_counts = df.isnull().sum()\n",
    "            if null_counts.any():\n",
    "                print(f\"\\n‚ö†Ô∏è Null Values:\")\n",
    "                for col, count in null_counts[null_counts > 0].items():\n",
    "                    print(f\"   ‚Ä¢ {col}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error reading file: {e}\")\n",
    "            return None\n",
    "\n",
    "    def select_folders_to_process(self, folder_range=\"all\"):\n",
    "        \"\"\"Select folders for processing\"\"\"\n",
    "        all_folders = self.get_sorted_folders()\n",
    "        print(f\"Total folders: {len(all_folders)}\")\n",
    "\n",
    "        selected_folders = []\n",
    "\n",
    "        if folder_range == \"all\":\n",
    "            selected_folders = all_folders\n",
    "            print(f\"‚úì All {len(selected_folders)} folders selected\")\n",
    "\n",
    "        elif isinstance(folder_range, int):\n",
    "            if 1 <= folder_range <= len(all_folders):\n",
    "                selected_folders = [all_folders[folder_range - 1]]\n",
    "                print(\n",
    "                    f\"‚úì Folder number {folder_range} selected: {selected_folders[0].name}\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"‚úó Invalid folder number: {folder_range}\")\n",
    "\n",
    "        elif isinstance(folder_range, tuple) and len(folder_range) == 2:\n",
    "            start, end = folder_range\n",
    "            if 1 <= start <= end <= len(all_folders):\n",
    "                selected_folders = all_folders[start - 1 : end]\n",
    "                print(\n",
    "                    f\"‚úì {len(selected_folders)} folders selected from {start} to {end}\"\n",
    "                )\n",
    "                print(f\"  From: {selected_folders[0].name}\")\n",
    "                print(f\"  To: {selected_folders[-1].name}\")\n",
    "            else:\n",
    "                print(f\"‚úó Invalid range: {start} to {end}\")\n",
    "\n",
    "        elif isinstance(folder_range, str) and \":\" in folder_range:\n",
    "            start_date, end_date = folder_range.split(\":\")\n",
    "            selected_folders = [\n",
    "                f for f in all_folders if start_date <= f.name <= end_date\n",
    "            ]\n",
    "            if selected_folders:\n",
    "                print(f\"‚úì {len(selected_folders)} folders selected in date range\")\n",
    "                print(f\"  From: {selected_folders[0].name}\")\n",
    "                print(f\"  To: {selected_folders[-1].name}\")\n",
    "            else:\n",
    "                print(f\"‚úó No folder found between {start_date} and {end_date}\")\n",
    "\n",
    "        elif isinstance(folder_range, str):\n",
    "            selected_folders = [f for f in all_folders if f.name == folder_range]\n",
    "            if selected_folders:\n",
    "                print(f\"‚úì Folder '{folder_range}' selected\")\n",
    "            else:\n",
    "                print(f\"‚úó Folder '{folder_range}' not found\")\n",
    "\n",
    "        return selected_folders\n",
    "\n",
    "    def read_parquet_safe(self, file_path):\n",
    "        \"\"\"Read parquet file with error handling\"\"\"\n",
    "        try:\n",
    "            df = pd.read_parquet(file_path, engine=\"fastparquet\")\n",
    "            return df\n",
    "        except:\n",
    "            try:\n",
    "                df = pd.read_parquet(file_path, engine=\"pyarrow\")\n",
    "                return df\n",
    "            except:\n",
    "                try:\n",
    "                    import pyarrow.parquet as pq\n",
    "\n",
    "                    table = pq.read_table(file_path)\n",
    "                    df = table.to_pandas(ignore_metadata=True)\n",
    "                    return df\n",
    "                except:\n",
    "                    return None\n",
    "\n",
    "    def process_folders(self, folder_range=\"all\", sample_size=None):\n",
    "        \"\"\"Process selected folders\"\"\"\n",
    "        selected_folders = self.select_folders_to_process(folder_range)\n",
    "\n",
    "        if not selected_folders:\n",
    "            print(\"No folder was selected for processing!\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\nStarting processing of {len(selected_folders)} folders...\")\n",
    "\n",
    "        for folder_idx, folder in enumerate(selected_folders, 1):\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Processing folder {folder_idx}/{len(selected_folders)}: {folder.name}\")\n",
    "            print(f\"{'='*50}\")\n",
    "\n",
    "            parquet_files = list(folder.glob(\"*.parquet\"))\n",
    "\n",
    "            if sample_size and sample_size < len(parquet_files):\n",
    "                parquet_files = parquet_files[:sample_size]\n",
    "                print(\n",
    "                    f\"üìå Only {sample_size} files out of {len(list(folder.glob('*.parquet')))} files will be processed\"\n",
    "                )\n",
    "\n",
    "            success_count = 0\n",
    "\n",
    "            for file_idx, file_path in enumerate(parquet_files, 1):\n",
    "                if file_idx % 100 == 0:\n",
    "                    print(f\"  Processing file {file_idx}/{len(parquet_files)}...\")\n",
    "\n",
    "                prefix, file_number = self.extract_file_components(file_path.stem)\n",
    "                df = self.read_parquet_safe(file_path)\n",
    "\n",
    "                if df is not None:\n",
    "                    df[\"source_date\"] = folder.name\n",
    "                    df[\"source_file\"] = file_path.stem\n",
    "                    df[\"file_prefix\"] = prefix\n",
    "                    df[\"file_number\"] = file_number\n",
    "                    df[\"folder_order\"] = folder_idx\n",
    "\n",
    "                    if prefix not in self.data_by_prefix:\n",
    "                        self.data_by_prefix[prefix] = []\n",
    "\n",
    "                    self.data_by_prefix[prefix].append(\n",
    "                        {\n",
    "                            \"date\": folder.name,\n",
    "                            \"folder_order\": folder_idx,\n",
    "                            \"file_number\": file_number,\n",
    "                            \"data\": df,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    success_count += 1\n",
    "                else:\n",
    "                    self.failed_files.append(\n",
    "                        {\"folder\": folder.name, \"file\": file_path.name}\n",
    "                    )\n",
    "\n",
    "            print(f\"  ‚úì {success_count} files read out of {len(parquet_files)}\")\n",
    "            gc.collect()\n",
    "\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing completed!\")\n",
    "        print(f\"  - Folders processed: {len(selected_folders)}\")\n",
    "        print(f\"  - Prefix groups detected: {len(self.data_by_prefix)}\")\n",
    "        for prefix, items in self.data_by_prefix.items():\n",
    "            print(f\"    ‚Ä¢ {prefix}: {len(items)} files\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "    def save_current_batch(self, output_path, batch_name=None):\n",
    "        \"\"\"Save currently processed data\"\"\"\n",
    "        output_path = Path(output_path)\n",
    "        output_path.mkdir(exist_ok=True)\n",
    "\n",
    "        if not batch_name:\n",
    "            batch_name = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "        batch_folder = output_path / f\"batch_{batch_name}\"\n",
    "        batch_folder.mkdir(exist_ok=True)\n",
    "\n",
    "        for prefix, items in self.data_by_prefix.items():\n",
    "            sorted_items = sorted(\n",
    "                items, key=lambda x: (x[\"folder_order\"], x[\"file_number\"])\n",
    "            )\n",
    "            dfs = [item[\"data\"] for item in sorted_items]\n",
    "\n",
    "            if dfs:\n",
    "                combined_df = pd.concat(dfs, ignore_index=True)\n",
    "                safe_prefix = prefix.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "\n",
    "                combined_df.to_csv(batch_folder / f\"{safe_prefix}.csv\", index=False)\n",
    "                print(f\"‚úì Saved: {safe_prefix}.csv ({len(combined_df)} rows)\")\n",
    "\n",
    "        if self.failed_files:\n",
    "            pd.DataFrame(self.failed_files).to_csv(\n",
    "                batch_folder / \"failed_files.csv\", index=False\n",
    "            )\n",
    "\n",
    "        print(f\"\\n‚úì All data saved in '{batch_folder}'\")\n",
    "        return batch_folder\n",
    "\n",
    "    def clear_memory(self):\n",
    "        \"\"\"Clear memory\"\"\"\n",
    "        self.data_by_prefix = {}\n",
    "        self.all_data = {}\n",
    "        gc.collect()\n",
    "        print(\"‚úì Memory cleared\")\n",
    "\n",
    "\n",
    "def combine_batches(batch_folder_path):\n",
    "    \"\"\"Combine all saved batches\"\"\"\n",
    "    batch_folder = Path(batch_folder_path)\n",
    "    all_batches = list(batch_folder.glob(\"batch_*\"))\n",
    "\n",
    "    combined_data = {}\n",
    "\n",
    "    for batch in all_batches:\n",
    "        print(f\"Reading {batch.name}...\")\n",
    "        csv_files = list(batch.glob(\"*.csv\"))\n",
    "\n",
    "        for csv_file in csv_files:\n",
    "            if csv_file.name != \"failed_files.csv\":\n",
    "                prefix = csv_file.stem\n",
    "                df = pd.read_csv(csv_file)\n",
    "\n",
    "                if prefix not in combined_data:\n",
    "                    combined_data[prefix] = []\n",
    "                combined_data[prefix].append(df)\n",
    "\n",
    "    final_data = {}\n",
    "    for prefix, dfs in combined_data.items():\n",
    "        final_data[prefix] = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"‚úì {prefix}: {len(final_data[prefix])} rows\")\n",
    "\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d882cfe8-8eac-4d5a-9cf1-c270c0b83dec",
   "metadata": {},
   "source": [
    "### Step 2 : Initial data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09278c02-4fa4-4ea9-9b80-12c1cf6691c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path\n",
    "base_path = \"../tennis_data\"\n",
    "\n",
    "# Create processor\n",
    "processor = TennisDataProcessor(base_path)\n",
    "\n",
    "# Show folders\n",
    "processor.show_available_folders(show_count=3)\n",
    "\n",
    "# Find missing files\n",
    "missing_report = processor.find_missing_files()\n",
    "\n",
    "# Inspect a sample file\n",
    "df_sample = processor.inspect_parquet_file(folder_idx=1, file_prefix=\"away_team\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff42564-500f-43dc-b1e1-23c9ad06df6b",
   "metadata": {},
   "source": [
    "### Step 3 : Process 60 folders in 6 batches of 10 (execution may take some time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bd48c3-7663-4106-9138-8b81ff308695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing 60 folders in 6 batches of 10\n",
    "base_path = \"../tennis_data\"\n",
    "output_path = \"../CSV_Files\"\n",
    "\n",
    "for batch_num in range(6):\n",
    "    start = batch_num * 10 + 1\n",
    "    end = min((batch_num + 1) * 10, 60)\n",
    "\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"Processing batch {batch_num + 1}: folders {start} to {end}\")\n",
    "    print(f\"{'#'*60}\")\n",
    "\n",
    "    processor = TennisDataProcessor(base_path)\n",
    "    processor.process_folders(folder_range=(start, end))\n",
    "    processor.save_current_batch(output_path, batch_name=f\"folders_{start}_{end}\")\n",
    "    processor.clear_memory()\n",
    "\n",
    "    print(f\"‚úì Batch {batch_num + 1} completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7621ab1-abbd-46d3-b1be-32a2fa853304",
   "metadata": {},
   "source": [
    "### Step 4 : Final merging and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea1c26e-28ec-4775-b6d4-b3f86fc5e876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_final_data(final_data, output_path=\"./final_data\", format=\"parquet\"):\n",
    "    \"\"\"\n",
    "    Save final data with error handling\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    final_data: dict of DataFrames\n",
    "    output_path: save path\n",
    "    format: 'parquet', 'csv', or 'both'\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    output_dir = Path(output_path)\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìÅ Saving data to: {output_dir.absolute()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    successful_saves = []\n",
    "    failed_saves = []\n",
    "\n",
    "    for prefix, df in final_data.items():\n",
    "        print(f\"\\nüìù Processing {prefix}...\")\n",
    "        print(f\"   ‚Ä¢ Rows: {len(df):,}\")\n",
    "        print(f\"   ‚Ä¢ Columns: {len(df.columns)}\")\n",
    "        print(f\"   ‚Ä¢ Approx Size: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "        success = False\n",
    "\n",
    "        # Save as parquet\n",
    "        if format in [\"parquet\", \"both\"]:\n",
    "            parquet_file = output_dir / f\"{prefix}_complete.parquet\"\n",
    "            try:\n",
    "                df.to_parquet(parquet_file)\n",
    "                print(f\"   ‚úì Parquet saved: {parquet_file.name}\")\n",
    "                success = True\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚úó Error saving Parquet: {e}\")\n",
    "\n",
    "        # Save as CSV\n",
    "        if format in [\"csv\", \"both\"] or (format == \"parquet\" and not success):\n",
    "            csv_file = output_dir / f\"{prefix}_complete.csv\"\n",
    "            try:\n",
    "                df.to_csv(csv_file, index=False)\n",
    "                print(f\"   ‚úì CSV saved: {csv_file.name}\")\n",
    "                success = True\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚úó Error saving CSV: {e}\")\n",
    "\n",
    "        if success:\n",
    "            successful_saves.append(prefix)\n",
    "        else:\n",
    "            failed_saves.append(prefix)\n",
    "\n",
    "    # Final report\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìä Final Report:\")\n",
    "    print(f\"   ‚úì Successful: {len(successful_saves)} files\")\n",
    "    print(f\"   ‚úó Failed: {len(failed_saves)} files\")\n",
    "\n",
    "    if failed_saves:\n",
    "        print(f\"\\n‚ö†Ô∏è Failed files:\")\n",
    "        for prefix in failed_saves:\n",
    "            print(f\"   - {prefix}\")\n",
    "\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    return successful_saves, failed_saves\n",
    "\n",
    "\n",
    "# Load combined data\n",
    "final_data = combine_batches(\"../CSV_Files\")\n",
    "\n",
    "# Save output in parquet format\n",
    "successful, failed = save_final_data(\n",
    "    final_data,\n",
    "    output_path=\"../Final_parquet\",\n",
    "    format=\"parquet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f82af16-8ad0-4739-90ae-4414d207dd77",
   "metadata": {},
   "source": [
    "### Step 5 : Verify data saving integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133b6f3f-0769-43b4-acbe-c41dcd5b8468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_final_data(\n",
    "    directory_path=\"../Final_parquet\",\n",
    "    file_format=\"parquet\",\n",
    "):\n",
    "    \"\"\"Load saved files\"\"\"\n",
    "    dir_path = Path(directory_path)\n",
    "\n",
    "    if not dir_path.exists():\n",
    "        print(f\"‚ùå Folder {directory_path} does not exist\")\n",
    "        return {}\n",
    "\n",
    "    loaded_data = {}\n",
    "    extension = \".parquet\" if file_format == \"parquet\" else \".csv\"\n",
    "\n",
    "    files = list(dir_path.glob(f\"*{extension}\"))\n",
    "\n",
    "    print(f\"üìñ Reading {len(files)} {file_format} files...\")\n",
    "\n",
    "    for file in files:\n",
    "        prefix = file.stem.replace(\"_complete\", \"\")\n",
    "\n",
    "        try:\n",
    "            if file_format == \"parquet\":\n",
    "                df = pd.read_parquet(file)\n",
    "            else:\n",
    "                df = pd.read_csv(file)\n",
    "\n",
    "            loaded_data[prefix] = df\n",
    "            print(f\"   ‚úì {prefix}: {len(df):,} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚úó Error reading {file.name}: {e}\")\n",
    "\n",
    "    return loaded_data\n",
    "\n",
    "\n",
    "# Load data\n",
    "loaded_data = load_final_data(\n",
    "    \"../Final_parquet\",\n",
    "    file_format=\"parquet\"\n",
    ")\n",
    "\n",
    "# Check sample\n",
    "if \"away_team\" in loaded_data:\n",
    "    print(f\"\\nSample data for away_team:\")\n",
    "    print(loaded_data[\"away_team\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9ce9cf-8f4a-40a9-898d-7017f69da71c",
   "metadata": {},
   "source": [
    "### Step 6 : Create dataset for analysis and data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172bbdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TennisDataExplorer:\n",
    "    def __init__(\n",
    "        self, data_path=\"../Final_parquet\"\n",
    "    ):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.datasets = {}\n",
    "        self.metadata = {}\n",
    "\n",
    "    def load_all_datasets(self):\n",
    "        \"\"\"Load all datasets\"\"\"\n",
    "        files = {\n",
    "            \"away_team\": \"away_team_complete.parquet\",\n",
    "            \"pbp\": \"pbp_complete.parquet\",\n",
    "            \"away_team_score\": \"away_team_score_complete.parquet\",\n",
    "            \"event\": \"event_complete.parquet\",\n",
    "            \"home_team\": \"home_team_complete.parquet\",\n",
    "            \"home_team_score\": \"home_team_score_complete.parquet\",\n",
    "            \"round\": \"round_complete.parquet\",\n",
    "            \"season\": \"season_complete.parquet\",\n",
    "            \"time\": \"time_complete.parquet\",\n",
    "            \"tournament\": \"tournament_complete.parquet\",\n",
    "            \"venue\": \"venue_complete.parquet\",\n",
    "            \"odds\": \"odds_complete.parquet\",\n",
    "            \"statistics\": \"statistics_complete.parquet\",\n",
    "            \"power\": \"power_complete.parquet\",\n",
    "            \"votes\": \"votes_complete.parquet\",\n",
    "        }\n",
    "\n",
    "        print(\"üîÑ Loading dataframes and aggregating into dataset\")\n",
    "        for name, filename in files.items():\n",
    "            file_path = self.data_path / filename\n",
    "            if file_path.exists():\n",
    "                try:\n",
    "                    # Only a few initial rows for quick check\n",
    "                    self.datasets[name] = pd.read_parquet(file_path)\n",
    "                    print(f\"   ‚úì {name}: {len(self.datasets[name]):,} rows\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚úó Error loading {name}: {e}\")\n",
    "\n",
    "        return self.datasets\n",
    "\n",
    "    def analyze_dataset_structure(self):\n",
    "        \"\"\"Analyze structure of all datasets\"\"\"\n",
    "        analysis_report = {}\n",
    "\n",
    "        for name, df in self.datasets.items():\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"üìä Analysis {name}\")\n",
    "            print(f\"{'='*60}\")\n",
    "\n",
    "            analysis = {\n",
    "                \"shape\": df.shape,\n",
    "                \"columns\": list(df.columns),\n",
    "                \"dtypes\": df.dtypes.to_dict(),\n",
    "                \"null_counts\": df.isnull().sum().to_dict(),\n",
    "                \"memory_usage\": df.memory_usage(deep=True).sum() / 1024**2,  # MB\n",
    "                \"sample_data\": df.head(3).to_dict(),\n",
    "            }\n",
    "\n",
    "            # Show key info\n",
    "            print(f\"üìè Dimensions: {analysis['shape'][0]:,} √ó {analysis['shape'][1]}\")\n",
    "            print(f\"üíæ Memory: {analysis['memory_usage']:.2f} MB\")\n",
    "            print(f\"üìã Columns: {', '.join(analysis['columns'][:5])}\")\n",
    "            if len(analysis[\"columns\"]) > 5:\n",
    "                print(f\"           ... and {len(analysis['columns'])-5} more columns\")\n",
    "\n",
    "            # Check potential key columns\n",
    "            key_columns = []\n",
    "            for col in df.columns:\n",
    "                if any(\n",
    "                    key in col.lower()\n",
    "                    for key in [\"id\", \"match\", \"game\", \"player\", \"team\"]\n",
    "                ):\n",
    "                    key_columns.append(col)\n",
    "                    unique_count = df[col].nunique()\n",
    "                    print(f\"   üîë {col}: {unique_count:,} unique values\")\n",
    "\n",
    "            analysis[\"potential_keys\"] = key_columns\n",
    "            analysis_report[name] = analysis\n",
    "\n",
    "            # Show sample data\n",
    "            print(f\"\\nüìù Sample data:\")\n",
    "            print(df.head(2).to_string())\n",
    "\n",
    "        self.metadata = analysis_report\n",
    "        return analysis_report\n",
    "\n",
    "    def find_relationships(self):\n",
    "        \"\"\"Find relationships between datasets\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üîó Checking relationships between datasets\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        relationships = {}\n",
    "\n",
    "        # Collect all columns\n",
    "        all_columns = {}\n",
    "        for name, df in self.datasets.items():\n",
    "            all_columns[name] = set(df.columns)\n",
    "\n",
    "        # Find common columns\n",
    "        for name1 in self.datasets:\n",
    "            relationships[name1] = {}\n",
    "            for name2 in self.datasets:\n",
    "                if name1 != name2:\n",
    "                    common_cols = all_columns[name1].intersection(all_columns[name2])\n",
    "                    if common_cols:\n",
    "                        relationships[name1][name2] = list(common_cols)\n",
    "\n",
    "        # Show relationships\n",
    "        for dataset, relations in relationships.items():\n",
    "            if relations:\n",
    "                print(f\"\\nüìä {dataset}:\")\n",
    "                for related, columns in relations.items():\n",
    "                    if columns:\n",
    "                        print(f\"   ‚ÜîÔ∏è {related}: {', '.join(columns[:3])}\")\n",
    "\n",
    "        return relationships\n",
    "\n",
    "    def create_data_profile(self):\n",
    "        \"\"\"Create complete data profile\"\"\"\n",
    "        profile = {\n",
    "            \"overview\": {},\n",
    "            \"columns_info\": {},\n",
    "            \"relationships\": {},\n",
    "            \"recommendations\": [],\n",
    "        }\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üìà Creating data profile\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Overall info\n",
    "        total_rows = sum(len(df) for df in self.datasets.values())\n",
    "        total_columns = sum(len(df.columns) for df in self.datasets.values())\n",
    "        total_memory = (\n",
    "            sum(df.memory_usage(deep=True).sum() for df in self.datasets.values())\n",
    "            / 1024**2\n",
    "        )\n",
    "\n",
    "        profile[\"overview\"] = {\n",
    "            \"total_datasets\": len(self.datasets),\n",
    "            \"total_rows\": total_rows,\n",
    "            \"total_columns\": total_columns,\n",
    "            \"total_memory_mb\": total_memory,\n",
    "        }\n",
    "\n",
    "        print(f\"\\nüìä Overall summary:\")\n",
    "        print(f\"   ‚Ä¢ Number of datasets: {len(self.datasets)}\")\n",
    "        print(f\"   ‚Ä¢ Total rows: {total_rows:,}\")\n",
    "        print(f\"   ‚Ä¢ Total columns: {total_columns}\")\n",
    "        print(f\"   ‚Ä¢ Total size: {total_memory:.2f} MB\")\n",
    "\n",
    "        # Deeper analysis for key datasets\n",
    "        key_datasets = [\"event\", \"tournament\", \"pbp\", \"statistics\"]\n",
    "\n",
    "        for ds_name in key_datasets:\n",
    "            if ds_name in self.datasets:\n",
    "                df = self.datasets[ds_name]\n",
    "                print(f\"\\nüîç Deep analysis {ds_name}:\")\n",
    "\n",
    "                # Check date columns\n",
    "                date_cols = [\n",
    "                    col\n",
    "                    for col in df.columns\n",
    "                    if \"date\" in col.lower() or \"time\" in col.lower()\n",
    "                ]\n",
    "                if date_cols:\n",
    "                    for col in date_cols[:2]:\n",
    "                        try:\n",
    "                            print(f\"   üìÖ {col}: from {df[col].min()} to {df[col].max()}\")\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                # Check ID columns\n",
    "                id_cols = [col for col in df.columns if \"id\" in col.lower()]\n",
    "                if id_cols:\n",
    "                    for col in id_cols[:3]:\n",
    "                        print(f\"   üîë {col}: {df[col].nunique():,} unique values\")\n",
    "\n",
    "        return profile\n",
    "\n",
    "    def generate_sample_queries(self):\n",
    "        \"\"\"Generate useful sample queries\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üîç Sample possible analyses\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        queries = []\n",
    "\n",
    "        # Based on available datasets\n",
    "        if \"tournament\" in self.datasets:\n",
    "            queries.append(\"Analyze different tournaments and number of matches\")\n",
    "\n",
    "        if \"statistics\" in self.datasets:\n",
    "            queries.append(\"Analyze players' performance statistics over time\")\n",
    "\n",
    "        if \"pbp\" in self.datasets:\n",
    "            queries.append(\"Analyze play-by-play details and scoring patterns\")\n",
    "\n",
    "        if \"odds\" in self.datasets:\n",
    "            queries.append(\"Evaluate prediction accuracy based on odds\")\n",
    "\n",
    "        if \"power\" in self.datasets:\n",
    "            queries.append(\"Analyze shot power and its impact on match outcome\")\n",
    "\n",
    "        for i, query in enumerate(queries, 1):\n",
    "            print(f\"   {i}. {query}\")\n",
    "\n",
    "        return queries\n",
    "\n",
    "\n",
    "# Run initial analysis\n",
    "explorer = TennisDataExplorer(\"../Final_parquet\")\n",
    "datasets = explorer.load_all_datasets()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tennis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
